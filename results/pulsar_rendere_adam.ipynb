{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "XATE3-AzioFP",
        "p1efWNPoiuNo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gJE61_q4M3p2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942a5f38-8ec1-401e-bccb-eafc813ebaf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.9.0)\n",
            "Collecting portalocker (from iopath)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=1ea939031e15305c4c5dcd800daa9ca956235095e254213abdeaa8171540c608\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=fd180c9fc7e9512d347870256eba60fd499d2b51020b887f579e4d24c4ee37c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/download.html\n",
            "Collecting pytorch3d\n",
            "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/pytorch3d-0.7.5-cp310-cp310-linux_x86_64.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (1.23.5)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.9.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (2.8.2)\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"2.1.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib==3.5.2\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "#!pip install torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1\n",
        "\n",
        "!pip install kaolin==0.15.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.1.0_cu121.html\n",
        "!pip install open3d"
      ],
      "metadata": {
        "id": "8CweaswYD-hZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9fbf8f3-7e0a-4c1a-cd24-b0290d8b673b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==3.5.2\n",
            "  Downloading matplotlib-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.2) (1.16.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "plotnine 0.12.4 requires matplotlib>=3.6.0, but you have matplotlib 3.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-9h_0pua9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-9h_0pua9\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=fad021525a6fb593870b621cb90863645834639bc46e3eed634fd99dc38421e7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-evszv8p_/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.3\n",
            "Looking in links: https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.1.0_cu121.html\n",
            "Collecting kaolin==0.15.0\n",
            "  Downloading https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.1.0_cu121/kaolin-0.15.0-cp310-cp310-linux_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipycanvas (from kaolin==0.15.0)\n",
            "  Downloading ipycanvas-0.13.1-py2.py3-none-any.whl (255 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.7/255.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipyevents in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (2.0.2)\n",
            "Requirement already satisfied: jupyter-client<8 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (6.1.12)\n",
            "Requirement already satisfied: pyzmq<25 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (23.2.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (2.2.5)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (6.3.2)\n",
            "Collecting comm>=0.1.3 (from kaolin==0.15.0)\n",
            "  Downloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (1.23.5)\n",
            "Collecting pybind11 (from kaolin==0.15.0)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (9.4.0)\n",
            "Requirement already satisfied: tqdm>=4.51.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (1.11.4)\n",
            "Collecting pygltflib (from kaolin==0.15.0)\n",
            "  Downloading pygltflib-1.16.1.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting usd-core<=23.5 (from kaolin==0.15.0)\n",
            "  Downloading usd_core-23.5-cp310-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from kaolin==0.15.0) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4 in /usr/local/lib/python3.10/dist-packages (from comm>=0.1.3->kaolin==0.15.0) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.15.0) (5.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.15.0) (2.8.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.15.0) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.15.0) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.15.0) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.15.0) (8.1.7)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from ipycanvas->kaolin==0.15.0) (7.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.15.0) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->kaolin==0.15.0)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.15.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.15.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.15.0) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.15.0) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.15.0) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.15.0) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.15.0) (4.9.0)\n",
            "Collecting dataclasses-json>=0.0.25 (from pygltflib->kaolin==0.15.0)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated (from pygltflib->kaolin==0.15.0)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.15.0)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.15.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (3.0.9)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->kaolin==0.15.0) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->kaolin==0.15.0) (2.1.5)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client<8->kaolin==0.15.0) (4.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->kaolin==0.15.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kaolin==0.15.0) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client<8->kaolin==0.15.0) (1.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pygltflib->kaolin==0.15.0) (1.14.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.15.0) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.15.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.15.0) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (6.5.5)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.19.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.0.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (4.19.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.17.1)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.15.0) (2.21)\n",
            "Building wheels for collected packages: pygltflib\n",
            "  Building wheel for pygltflib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygltflib: filename=pygltflib-1.16.1-py3-none-any.whl size=27085 sha256=055c48cb45581e036b1d94d98460f2462eee3aaf48fbdd022a6ad40304fa022e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/73/91/ae3a93bb6cc6dde662ed1dc48dd0ec7ca41a7bcc2a1a99b5a6\n",
            "Successfully built pygltflib\n",
            "Installing collected packages: usd-core, pybind11, mypy-extensions, marshmallow, jedi, deprecated, comm, typing-inspect, dataclasses-json, pygltflib, ipycanvas, kaolin\n",
            "Successfully installed comm-0.2.1 dataclasses-json-0.6.4 deprecated-1.2.14 ipycanvas-0.13.1 jedi-0.19.1 kaolin-0.15.0 marshmallow-3.20.2 mypy-extensions-1.0.0 pybind11-2.11.1 pygltflib-1.16.1 typing-inspect-0.9.0 usd-core-23.5\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl (399.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.23.5)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-2.15.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.1)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.9.2)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (9.4.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.5.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.5.3)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.66.1)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.2.5)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.15.0)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.31.0)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (67.7.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (7.0.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.10 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab-widgets~=3.0.10 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (2.1.5)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.17.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.17.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat>=5.7.0->open3d) (4.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.2.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, addict, widgetsnbextension, retrying, pyquaternion, jupyterlab-widgets, configargparse, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.6\n",
            "    Uninstalling widgetsnbextension-3.6.6:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.6\n",
            "  Attempting uninstall: jupyterlab-widgets\n",
            "    Found existing installation: jupyterlab-widgets 3.0.9\n",
            "    Uninstalling jupyterlab-widgets-3.0.9:\n",
            "      Successfully uninstalled jupyterlab-widgets-3.0.9\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 configargparse-1.7 dash-2.15.0 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 open3d-0.18.0 pyquaternion-0.9.9 retrying-1.3.4 widgetsnbextension-4.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from math import radians\n",
        "from pytorch3d.transforms import axis_angle_to_matrix\n",
        "# Util function for loading point clouds|\n",
        "import numpy as np\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Pointclouds\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_rotation,\n",
        "    look_at_view_transform,\n",
        "    FoVOrthographicCameras,\n",
        "    PointsRasterizationSettings,\n",
        "    PointsRenderer,\n",
        "    PulsarPointsRenderer,\n",
        "    PointsRasterizer,\n",
        "    AlphaCompositor,\n",
        "    NormWeightedCompositor,\n",
        "    FoVPerspectiveCameras,\n",
        "    PerspectiveCameras,\n",
        "    rotate_on_spot,\n",
        "    OrthographicCameras,\n",
        "    PointLights,\n",
        ")"
      ],
      "metadata": {
        "id": "6bjzU3rLNHAm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "def render_point_cloud_2(point_cloud,distance,elev,azim,a1,a2,a3):\n",
        "\n",
        "  R, T = look_at_view_transform(distance, elev, azim)\n",
        "\n",
        "  #angles = [radians(a1), radians(a2),radians(a3)]\n",
        "  #rotation = axis_angle_to_matrix(torch.FloatTensor(angles))\n",
        "  #R, T = rotate_on_spot(R, T, rotation)\n",
        "  cameras = FoVPerspectiveCameras(device=device, R=R, T=T, znear=0.01)#questa la fa di fronte\n",
        "  # Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "  # 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "  # and blur_radius=0.0. Refer to raster_points.py for explanations of these parameters.\n",
        "  raster_settings = PointsRasterizationSettings(\n",
        "      image_size=224,\n",
        "      radius = 0.00001,\n",
        "      points_per_pixel = 1\n",
        "  )\n",
        "  renderer = PointsRenderer(\n",
        "      rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "      compositor=AlphaCompositor(background_color=(0, 0, 0)),\n",
        "\n",
        "  )\n",
        "\n",
        "  return renderer(point_cloud)\n",
        "def render_point_cloud(point_cloud,distance,elev,azim,a1,a2,a3):\n",
        "\n",
        "  R, T = look_at_view_transform(distance, elev, azim)\n",
        "\n",
        "  #angles = [radians(a1), radians(a2),radians(a3)]\n",
        "  #rotation = axis_angle_to_matrix(torch.FloatTensor(angles))\n",
        "  #R, T = rotate_on_spot(R, T, rotation)\n",
        "  cameras = FoVPerspectiveCameras(device=device, R=R, T=T, znear=0.01)#questa la fa di fronte\n",
        "  # Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "  # 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "  # and blur_radius=0.0. Refer to raster_points.py for explanations of these parameters.\n",
        "  raster_settings = PointsRasterizationSettings(\n",
        "      image_size=512,\n",
        "      radius = 0.00005,\n",
        "      points_per_pixel = 10\n",
        "  )\n",
        "\n",
        "  renderer = PulsarPointsRenderer(\n",
        "      rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "      n_channels=3\n",
        "  ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #return renderer(point_cloud, gamma=(1e-5,),bg_col=torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32, device=device))\n",
        "  return renderer(point_cloud, gamma=(1e-5,))\n",
        "\n",
        "def resize(image, res):\n",
        "  resized=image.permute(3, 1, 2, 0).squeeze(3)\n",
        "  transform = transforms.Resize((res, res))\n",
        "  out=transform(resized)\n",
        "  return out\n",
        "\n",
        "\n",
        "def generate_static_views_old(point_cloud, res):\n",
        "  #elev da -90 a 90\n",
        "  #azimnut da 0 a 360\n",
        "  #angoli da 0 a 360\n",
        "  images = [ resize(render_point_cloud(point_cloud,3,0,0,0,0,0),res),\n",
        "             resize(render_point_cloud(point_cloud,3,0,0,0,0,540),res),\n",
        "             resize(render_point_cloud(point_cloud,3,90,0,0,0,0),res),\n",
        "             resize(render_point_cloud(point_cloud,3,-80,-2,0,0,0),res),\n",
        "             resize(render_point_cloud(point_cloud,3,0,50,0,0,0),res)\n",
        "  ]\n",
        "\n",
        "  images = torch.stack(images)\n",
        "  return images\n",
        "\n",
        "def generate_static_views(point_cloud, res):\n",
        "  #elev da -90 a 90           Fa ruotare  -90 = visto dall'altro    90 visto dal basso  e 0 visto difronte\n",
        "  #azimnut da 0 a 360         Fa ruotare  0 visto difronte  360 visto da dietro      -120 visto di lato\n",
        "  #angoli da 0 a 360\n",
        "    images = [ resize(render_point_cloud(point_cloud,3,90,-120,0,0,0),res)\n",
        "\n",
        "\n",
        "    ]\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    return images\n",
        "\n",
        "\n",
        "def generate_random_view(point_cloud, res, n_views, std=1):\n",
        "  elev = random.sample(range(-90, 91), n_views)  # Genera 4 elevazioni casuali tra -90 e 90\n",
        "  azimut = random.sample(range(0, 361), n_views)\n",
        "  images = []\n",
        "  for i in range(n_views):\n",
        "      images.append(resize(render_point_cloud(point_cloud,3,elev[i],azimut[i],0,0,0),res))\n",
        "\n",
        "  images = torch.stack(images)\n",
        "  return images\n",
        "\n"
      ],
      "metadata": {
        "id": "6bOG9VHnFk4T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "\n",
        "import open3d as o3d\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def segment2rgb(pred_class, colors):\n",
        "    pred_rgb = torch.zeros(pred_class.shape[0], 3).to(device)\n",
        "    for class_idx, color in enumerate(colors):\n",
        "        pred_rgb += torch.matmul(pred_class[:,class_idx].unsqueeze(1), color.unsqueeze(0))\n",
        "\n",
        "    return pred_rgb\n",
        "\n",
        "def color_point_cloud(pred_class, verts, colors):\n",
        "    pred_rgb = segment2rgb(pred_class, colors)\n",
        "    #print(pred_rgb.shape)\n",
        "    #print(verts.shape[0])\n",
        "\n",
        "    return Pointclouds(points=[verts], features=[pred_rgb])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5YtYt4U6NNmN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade pip\n",
        "!git clone https://github.com/rasenqt/AML_Project_2024.git\n",
        "#!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n"
      ],
      "metadata": {
        "id": "xZ8AyvM6_AmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7d4b97-bcce-490e-ec9b-ce978179bf02"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AML_Project_2024'...\n",
            "remote: Enumerating objects: 988, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 988 (delta 86), reused 205 (delta 77), pack-reused 774\u001b[K\n",
            "Receiving objects: 100% (988/988), 203.44 MiB | 13.66 MiB/s, done.\n",
            "Resolving deltas: 100% (253/253), done.\n",
            "Updating files: 100% (837/837), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZE POINT CLOUD\n"
      ],
      "metadata": {
        "id": "XATE3-AzioFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "sys.path.append('/content/AML_Project_2024/3DHighlighter-main')\n",
        "from itertools import permutations, product\n",
        "from neural_highlighter import NeuralHighlighter\n",
        "from Normalization import MeshNormalizer,PointCloudNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "import open3d as o3d\n",
        "\n",
        "\n",
        "\n",
        "def optimize_point_cloud(args):\n",
        "    # Constrain most sources of randomness\n",
        "    # (some torch backwards functions within CLIP are non-determinstic)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Load CLIP model\n",
        "    clip_model, preprocess = clip.load(args.clipmodel, device, jit=args.jit)\n",
        "\n",
        "    # Adjust output resolution depending on model type\n",
        "    res = 224\n",
        "    if args.clipmodel == \"ViT-L/14@336px\":\n",
        "        res = 336\n",
        "    if args.clipmodel == \"RN50x4\":\n",
        "        res = 288\n",
        "    if args.clipmodel == \"RN50x16\":\n",
        "        res = 384\n",
        "    if args.clipmodel == \"RN50x64\":\n",
        "        res = 448\n",
        "\n",
        "    Path(os.path.join(args.output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "    pointcloud = o3d.io.read_point_cloud(args.obj_path)\n",
        "    verts = torch.Tensor(np.asarray(pointcloud.points)).to(device)\n",
        "\n",
        "    print(\"Verts shape\")\n",
        "    print(verts.shape)\n",
        "\n",
        "\n",
        "    # Manually set RGB color (assuming you want to set all points to red)\n",
        "    manual_color = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32, device=device)\n",
        "\n",
        "    # Repeat the manual color for each vertex\n",
        "    rgb_colors = manual_color.repeat(verts.shape[0], 1)\n",
        "\n",
        "    shift = torch.mean(verts, dim=0)\n",
        "    scale = torch.max(torch.norm(verts-shift, p=2, dim=1))\n",
        "    verts=(verts-shift)/scale\n",
        "\n",
        "    point_cloud = Pointclouds(points=[verts], features=[rgb_colors])\n",
        "    #PointCloudNormalizer(point_cloud)()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize variables\n",
        "    background = None\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    n_augs = args.n_augs\n",
        "    dir = args.output_dir\n",
        "\n",
        "    # Record command line arguments\n",
        "    with open(os.path.join(dir, 'commandline_args.txt'), 'w') as f:\n",
        "        json.dump(args.__dict__, f, indent=2)\n",
        "\n",
        "    # CLIP and Augmentation Transforms\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # MLP Settings\n",
        "    mlp = NeuralHighlighter(args.depth, args.width, out_dim=args.n_classes, positional_encoding=args.positional_encoding,\n",
        "                            sigma=args.sigma).to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), args.learning_rate)\n",
        "\n",
        "    # list of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "    # --- Prompt ---\n",
        "    # pre-process multi_word_inputs\n",
        "    args.object[0] = ' '.join(args.object[0].split('_'))\n",
        "    for i in range(len(args.classes)):\n",
        "        args.classes[i] = ' '.join(args.classes[i].split('_'))\n",
        "    # encode prompt with CLIP\n",
        "    prompt = \"Point cloud depth map of gray {} with highlighted {}\".format(args.object[0], args.classes[0])\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(verts)\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(args.n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        verts_curr = vertices\n",
        "        colored_point_cloud = color_point_cloud(pred_class, verts_curr, colors)\n",
        "        rendered_images = generate_static_views(colored_point_cloud, res)\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(dir, i, rendered_images)\n",
        "            with open(os.path.join(dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "    # re-initialize background color\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    # save results\n",
        "    #save_final_results(args, dir, vertices, mlp, vertices, colors, render, background)\n",
        "\n",
        "    # Save prompts\n",
        "    with open(os.path.join(dir, prompt), \"w\") as f:\n",
        "        f.write('')\n",
        "\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(args, dir, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=args.n_views,\n",
        "                                                                        show=args.show,\n",
        "                                                                        center_azim=args.frontview_center[0],\n",
        "                                                                        center_elev=args.frontview_center[1],\n",
        "                                                                        std=args.frontview_std,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "        mesh.export(os.path.join(dir, f\"{objbase}_{args.classes[0]}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if args.n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if args.clipavg == \"view\":\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "        else:\n",
        "            loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    elif args.n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(args.n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if args.clipavg == \"view\":\n",
        "                if encoded_text.shape[0] > 1:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "                else:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "            else:\n",
        "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "y59DBoOSteAI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZE MESH"
      ],
      "metadata": {
        "id": "p1efWNPoiuNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from itertools import permutations, product\n",
        "from neural_highlighter import NeuralHighlighter\n",
        "from Normalization import MeshNormalizer,PointCloudNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "\n",
        "\n",
        "def optimize(args):\n",
        "    # Constrain most sources of randomness\n",
        "    # (some torch backwards functions within CLIP are non-determinstic)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Load CLIP model\n",
        "    clip_model, preprocess = clip.load(args.clipmodel, device, jit=args.jit)\n",
        "\n",
        "    # Adjust output resolution depending on model type\n",
        "    res = 224\n",
        "    if args.clipmodel == \"ViT-L/14@336px\":\n",
        "        res = 336\n",
        "    if args.clipmodel == \"RN50x4\":\n",
        "        res = 288\n",
        "    if args.clipmodel == \"RN50x16\":\n",
        "        res = 384\n",
        "    if args.clipmodel == \"RN50x64\":\n",
        "        res = 448\n",
        "\n",
        "    Path(os.path.join(args.output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "\n",
        "    render = Renderer(dim=(args.render_res, args.render_res))\n",
        "    mesh = Mesh(args.obj_path)\n",
        "    MeshNormalizer(mesh)()\n",
        "\n",
        "    # Initialize variables\n",
        "    background = None\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    n_augs = args.n_augs\n",
        "    dir = args.output_dir\n",
        "\n",
        "    # Record command line arguments\n",
        "    with open(os.path.join(dir, 'commandline_args.txt'), 'w') as f:\n",
        "        json.dump(args.__dict__, f, indent=2)\n",
        "\n",
        "    # CLIP and Augmentation Transforms\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # MLP Settings\n",
        "    mlp = NeuralHighlighter(args.depth, args.width, out_dim=args.n_classes, positional_encoding=args.positional_encoding,\n",
        "                            sigma=args.sigma).to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), args.learning_rate)\n",
        "\n",
        "    # list of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "    # --- Prompt ---\n",
        "    # pre-process multi_word_inputs\n",
        "    args.object[0] = ' '.join(args.object[0].split('_'))\n",
        "    for i in range(len(args.classes)):\n",
        "        args.classes[i] = ' '.join(args.classes[i].split('_'))\n",
        "    # encode prompt with CLIP\n",
        "    prompt = \"A 3D render of a gray {} with highlighted {}\".format(args.object[0], args.classes[0])\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(mesh.vertices)\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(args.n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        sampled_mesh = mesh\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "        rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=args.n_views,\n",
        "                                                                show=args.show,\n",
        "                                                                center_azim=args.frontview_center[0],\n",
        "                                                                center_elev=args.frontview_center[1],\n",
        "                                                                std=args.frontview_std,\n",
        "                                                                return_views=True,\n",
        "                                                                lighting=True,\n",
        "                                                                background=background)\n",
        "\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(dir, i, rendered_images)\n",
        "            with open(os.path.join(dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "    # re-initialize background color\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    # save results\n",
        "    save_final_results(args, dir, mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "    # Save prompts\n",
        "    with open(os.path.join(dir, prompt), \"w\") as f:\n",
        "        f.write('')\n",
        "\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(args, dir, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=args.n_views,\n",
        "                                                                        show=args.show,\n",
        "                                                                        center_azim=args.frontview_center[0],\n",
        "                                                                        center_elev=args.frontview_center[1],\n",
        "                                                                        std=args.frontview_std,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "        mesh.export(os.path.join(dir, f\"{objbase}_{args.classes[0]}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if args.n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if args.clipavg == \"view\":\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "        else:\n",
        "            loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    elif args.n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(args.n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if args.clipavg == \"view\":\n",
        "                if encoded_text.shape[0] > 1:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "                else:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "            else:\n",
        "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
      ],
      "metadata": {
        "id": "Ya33v0xMA5l4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZE AFFORDANCE"
      ],
      "metadata": {
        "id": "ROFtCSSXjDZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "sys.path.append('/content/AML_Project_2024/3DHighlighter-main')\n",
        "from itertools import permutations, product\n",
        "from neural_highlighter import NeuralHighlighter\n",
        "from Normalization import MeshNormalizer,PointCloudNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "import open3d as o3d\n",
        "\n",
        "\n",
        "\n",
        "def optimize_affordance(args, verts):\n",
        "    # Constrain most sources of randomness\n",
        "    # (some torch backwards functions within CLIP are non-determinstic)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Load CLIP model\n",
        "    clip_model, preprocess = clip.load(args.clipmodel, device, jit=args.jit)\n",
        "\n",
        "    # Adjust output resolution depending on model type\n",
        "    res = 224\n",
        "    if args.clipmodel == \"ViT-L/14@336px\":\n",
        "        res = 336\n",
        "    if args.clipmodel == \"RN50x4\":\n",
        "        res = 288\n",
        "    if args.clipmodel == \"RN50x16\":\n",
        "        res = 384\n",
        "    if args.clipmodel == \"RN50x64\":\n",
        "        res = 448\n",
        "\n",
        "    Path(os.path.join(args.output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "    verts = verts.to(device)\n",
        "\n",
        "    print(\"Verts shape\")\n",
        "    print(verts.shape)\n",
        "\n",
        "\n",
        "    # Manually set RGB color (assuming you want to set all points to red)\n",
        "    manual_color = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float32, device=device)\n",
        "\n",
        "    # Repeat the manual color for each vertex\n",
        "    rgb_colors = manual_color.repeat(verts.shape[0], 1)\n",
        "\n",
        "    shift = torch.mean(verts, dim=0)\n",
        "    scale = torch.max(torch.norm(verts-shift, p=2, dim=1))\n",
        "    verts=(verts-shift)/scale\n",
        "\n",
        "    point_cloud = Pointclouds(points=[verts], features=[rgb_colors])\n",
        "    #PointCloudNormalizer(point_cloud)()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize variables\n",
        "    background = None\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    n_augs = args.n_augs\n",
        "    dir = args.output_dir\n",
        "\n",
        "    # Record command line arguments\n",
        "    with open(os.path.join(dir, 'commandline_args.txt'), 'w') as f:\n",
        "        json.dump(args.__dict__, f, indent=2)\n",
        "\n",
        "    # CLIP and Augmentation Transforms\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # MLP Settings\n",
        "    mlp = NeuralHighlighter(args.depth, args.width, out_dim=args.n_classes, positional_encoding=args.positional_encoding,\n",
        "                            sigma=args.sigma).to(device)\n",
        "    momentum_value = 0.9  # Puoi regolare il valore del momentum\n",
        "    optim = torch.optim.Adam(mlp.parameters(), lr=args.learning_rate)\n",
        "    #optim = torch.optim.SGD(mlp.parameters(), lr=args.learning_rate, momentum=0.9)\n",
        "    #optim=torch.optim.AdamW(mlp.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "    #optim = torch.optim.Adam(mlp.parameters(), args.learning_rate)\n",
        "\n",
        "    # list of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    #full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]  #highlight gray\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    #rgb_to_color = {(0, 0., 255): \"blue\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    #color_to_rgb = {\"blue\": [0, 0., 255.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    #full_colors = [[0, 0., 255.], [180/255, 180/255, 180/255]]\n",
        "    #colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    #rgb_to_color = {(0, 0., 1.): \"blue\", (204/255, 1., 0.): \"highlighter\"}\n",
        "    #color_to_rgb = {\"blue\": [0, 0., 1.], \"highlighter\": [204/255, 1., 0.]}\n",
        "    #full_colors = [[1., 0., 1.],[204/255, 1., 0.]]\n",
        "    #colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # --- Prompt ---\n",
        "    # pre-process multi_word_inputs\n",
        "    args.object[0] = ' '.join(args.object[0].split('_'))\n",
        "    for i in range(len(args.classes)):\n",
        "        args.classes[i] = ' '.join(args.classes[i].split('_'))\n",
        "    # encode prompt with CLIP\n",
        "    #prompt = \"If you were to open the Door, from which point on the door would you open it?\"\n",
        "    #prompt = \"A point cloud representing a gray {} with highlighted  handle\".format(args.object[0], args.classes[0])\n",
        "    prompt = \"A point cloud representing a gray {} with highlighted  display\".format(args.object[0])\n",
        "    #prompt =\"point cloud depth map of a gray {} with hightlighted legs\".format(args.object[0])\n",
        "    #prompt =\"point cloud depth  of hightlighted legs of  a {} \".format(args.object[0])\n",
        "    #prompt=\"A point cloud of the highlighted legs of a chair\"\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(verts)\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(args.n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        verts_curr = vertices\n",
        "        colored_point_cloud = color_point_cloud(pred_class, verts_curr, colors)\n",
        "        rendered_images =  generate_random_view(colored_point_cloud, res, args.n_views)\n",
        "        #rendered_images =  generate_static_views(colored_point_cloud, res)\n",
        "\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(dir, i, rendered_images)\n",
        "            with open(os.path.join(dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "    # re-initialize background color\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    # save results\n",
        "    #save_final_results(args, dir, vertices, mlp, vertices, colors, render, background)\n",
        "\n",
        "    # Save prompts\n",
        "    with open(os.path.join(dir, prompt), \"w\") as f:\n",
        "        f.write('')\n",
        "\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(args, dir, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=args.n_views,\n",
        "                                                                        show=args.show,\n",
        "                                                                        center_azim=args.frontview_center[0],\n",
        "                                                                        center_elev=args.frontview_center[1],\n",
        "                                                                        std=args.frontview_std,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "        mesh.export(os.path.join(dir, f\"{objbase}_{args.classes[0]}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if args.n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if args.clipavg == \"view\":\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "        else:\n",
        "            loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    elif args.n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(args.n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if args.clipavg == \"view\":\n",
        "                if encoded_text.shape[0] > 1:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "                else:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "            else:\n",
        "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n",
        "\n"
      ],
      "metadata": {
        "id": "7yawqTO-jNpF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN AFFORDANCE"
      ],
      "metadata": {
        "id": "jUDtbwWpkFmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#sys.path.append('/content/AML_Project_2024/3DHighlighter-main/')\n",
        "#from main import optimize # Import the function from the module\n",
        "\n",
        "class ArgsNamespace:\n",
        "    def __init__(self, args_dict):\n",
        "        self.__dict__.update(args_dict)\n",
        "\n",
        "# Create an instance of ArgsNamespace\n",
        "\n",
        "\n",
        "\n",
        "# specify the directory you want to iterate over\n",
        "#directory = './AML_Project_2024/point_cloud_dataset/'  # current directory\n",
        "#directory= './AML_Project_2024/dataset/'\n",
        "directory = './AML_Project_2024/full_shape_val_data/Laptop/'\n",
        "# iterate over files in the directory\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "    with open(directory + filename, 'rb') as f: ## da definire...\n",
        "      # Load the object from the file\n",
        "      temp_data = pkl.load(f)\n",
        "      # print((temp_data[0]))\n",
        "      verts = temp_data[10][\"full_shape\"][\"coordinate\"]\n",
        "      verts = torch.Tensor(np.asarray(verts))\n",
        "\n",
        "\n",
        "    object_name = temp_data[10][\"semantic class\"]\n",
        "    print(object_name)\n",
        "\n",
        "\n",
        "    args = ArgsNamespace({\n",
        "        'seed': 0,\n",
        "        'obj_path': directory + filename,\n",
        "        'output_dir': '/content/ondaalta' + object_name,\n",
        "        'object': [object_name],\n",
        "        'classes': [\"knife\"],\n",
        "        'background': [1., 1., 1.],\n",
        "        'n_views': 5,\n",
        "        'frontview_std': 4,\n",
        "        'frontview_center': [0., 0.],\n",
        "        'show': False,\n",
        "        'n_augs': 3,\n",
        "        'clipavg': 'view',\n",
        "        'render_res': 224,\n",
        "        'clipmodel': 'ViT-L/14',\n",
        "        'jit': False,\n",
        "        'depth': 3,\n",
        "        'width': 256,\n",
        "        'n_classes': 2,\n",
        "        'positional_encoding': False,\n",
        "        'sigma': 3.0,\n",
        "        'learning_rate': 0.0001,\n",
        "        'n_iter': 2500\n",
        "    })\n",
        "\n",
        "\n",
        "    optimize_affordance(args, verts)\n",
        "    break\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/results_television.zip /content/results_television\n",
        "files.download('/content/results_television.zip')\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "m7fj4_qkf_I8",
        "outputId": "51c8223b-458c-4b6e-f3c3-7d10e8041d03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laptop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 890M/890M [00:09<00:00, 97.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verts shape\n",
            "torch.Size([2048, 3])\n",
            "ModuleList(\n",
            "  (0): Linear(in_features=3, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (7): ReLU()\n",
            "  (8): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (9): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (10): ReLU()\n",
            "  (11): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (12): Linear(in_features=256, out_features=2, bias=True)\n",
            "  (13): Softmax(dim=1)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/pytorch3d/renderer/points/pulsar/renderer.py:619: UserWarning: Extreme ratio of `max_depth` vs. focal length detected (100.000000 vs. 0.009999, ratio: 10001.000000). This will likely lead to artifacts due to numerical instabilities.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "  0%|          | 1/2500 [00:02<1:34:46,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.83984375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 101/2500 [01:08<26:34,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8740966796875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 201/2500 [02:15<26:13,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8871240234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 301/2500 [03:25<25:39,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8853955078125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 401/2500 [04:34<24:44,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8824072265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 501/2500 [05:44<23:39,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.88984375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 601/2500 [06:54<21:57,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.88806640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 701/2500 [08:03<20:45,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8908203125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 801/2500 [09:13<19:47,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8930126953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 901/2500 [10:22<18:49,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8814013671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 1001/2500 [11:32<17:20,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8912939453125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 1101/2500 [12:42<16:16,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.887724609375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 1201/2500 [13:51<15:06,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.88802734375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 1301/2500 [15:01<14:01,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.89060546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 1401/2500 [16:10<12:43,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8895703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1501/2500 [17:19<11:31,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.891220703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 1601/2500 [18:29<10:27,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8878759765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 1701/2500 [19:39<09:17,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8879931640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 1801/2500 [20:48<08:20,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8905908203125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 1901/2500 [21:58<06:56,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8901708984375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 2001/2500 [23:07<05:45,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8905078125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 2101/2500 [24:17<04:38,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.884482421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 2201/2500 [25:26<03:31,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.885869140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 2301/2500 [26:36<02:19,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.8930615234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 2401/2500 [27:46<01:08,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.88736328125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [28:54<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nfrom google.colab import files\\n\\n!zip -r /content/results_television.zip /content/results_television\\nfiles.download('/content/results_television.zip')\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLBL-0lttRTv",
        "outputId": "8d3203f9-d3af-4aa4-ad38-593255188579"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}