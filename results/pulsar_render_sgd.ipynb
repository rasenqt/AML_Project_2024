{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "XATE3-AzioFP",
        "p1efWNPoiuNo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJE61_q4M3p2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81254f75-43e8-447f-dcea-1938105e4a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.9.0)\n",
            "Collecting portalocker (from iopath)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=55de47df7dda82362d4514d631f61d64a128f31b29610259598a2755a09b4996\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=3ef11d66bcb047f5ae63b3f2bd2acd4449f1c9cf7d16183a542279b2674c09db\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"2.1.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib==3.5.2\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "#!pip install torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1\n",
        "\n",
        "!pip install kaolin==0.15.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.1.0_cu121.html\n",
        "!pip install open3d"
      ],
      "metadata": {
        "id": "8CweaswYD-hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from math import radians\n",
        "from pytorch3d.transforms import axis_angle_to_matrix\n",
        "# Util function for loading point clouds|\n",
        "import numpy as np\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Pointclouds\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_rotation,\n",
        "    look_at_view_transform,\n",
        "    FoVOrthographicCameras,\n",
        "    PointsRasterizationSettings,\n",
        "    PointsRenderer,\n",
        "    PulsarPointsRenderer,\n",
        "    PointsRasterizer,\n",
        "    AlphaCompositor,\n",
        "    NormWeightedCompositor,\n",
        "    FoVPerspectiveCameras,\n",
        "    PerspectiveCameras,\n",
        "    rotate_on_spot,\n",
        "    OrthographicCameras,\n",
        "    PointLights,\n",
        ")"
      ],
      "metadata": {
        "id": "6bjzU3rLNHAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "def render_point_cloud_2(point_cloud,distance,elev,azim,a1,a2,a3):\n",
        "\n",
        "  R, T = look_at_view_transform(distance, elev, azim)\n",
        "\n",
        "  #angles = [radians(a1), radians(a2),radians(a3)]\n",
        "  #rotation = axis_angle_to_matrix(torch.FloatTensor(angles))\n",
        "  #R, T = rotate_on_spot(R, T, rotation)\n",
        "  cameras = FoVPerspectiveCameras(device=device, R=R, T=T, znear=0.01)#questa la fa di fronte\n",
        "  # Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "  # 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "  # and blur_radius=0.0. Refer to raster_points.py for explanations of these parameters.\n",
        "  raster_settings = PointsRasterizationSettings(\n",
        "      image_size=224,\n",
        "      radius = 0.00001,\n",
        "      points_per_pixel = 1\n",
        "  )\n",
        "  renderer = PointsRenderer(\n",
        "      rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "      compositor=AlphaCompositor(background_color=(0, 0, 0)),\n",
        "\n",
        "  )\n",
        "\n",
        "  return renderer(point_cloud)\n",
        "def render_point_cloud(point_cloud,distance,elev,azim,a1,a2,a3):\n",
        "\n",
        "  R, T = look_at_view_transform(distance, elev, azim)\n",
        "\n",
        "  #angles = [radians(a1), radians(a2),radians(a3)]\n",
        "  #rotation = axis_angle_to_matrix(torch.FloatTensor(angles))\n",
        "  #R, T = rotate_on_spot(R, T, rotation)\n",
        "  cameras = FoVPerspectiveCameras(device=device, R=R, T=T, znear=0.01)#questa la fa di fronte\n",
        "  # Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "  # 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "  # and blur_radius=0.0. Refer to raster_points.py for explanations of these parameters.\n",
        "  raster_settings = PointsRasterizationSettings(\n",
        "      image_size=512,\n",
        "      radius = 0.00005,\n",
        "      points_per_pixel = 10\n",
        "  )\n",
        "\n",
        "  renderer = PulsarPointsRenderer(\n",
        "      rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "      n_channels=3\n",
        "  ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return renderer(point_cloud, gamma=(1e-5,),\n",
        "                  bg_col=torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32, device=device))\n",
        "\n",
        "def resize(image, res):\n",
        "  resized=image.permute(3, 1, 2, 0).squeeze(3)\n",
        "  transform = transforms.Resize((res, res))\n",
        "  out=transform(resized)\n",
        "  return out\n",
        "\n",
        "\n",
        "def generate_static_views_old(point_cloud, res):\n",
        "  #elev da -90 a 90\n",
        "  #azimnut da 0 a 360\n",
        "  #angoli da 0 a 360\n",
        "  images = [ resize(render_point_cloud(point_cloud,3,0,0,0,0,0),res),\n",
        "             resize(render_point_cloud(point_cloud,3,0,0,0,0,540),res),\n",
        "             resize(render_point_cloud(point_cloud,3,90,0,0,0,0),res),\n",
        "             resize(render_point_cloud(point_cloud,3,-80,-2,0,0,0),res),\n",
        "             resize(render_point_cloud(point_cloud,3,0,50,0,0,0),res)\n",
        "  ]\n",
        "\n",
        "  images = torch.stack(images)\n",
        "  return images\n",
        "\n",
        "def generate_static_views(point_cloud, res):\n",
        "  #elev da -90 a 90           Fa ruotare  -90 = visto dall'altro    90 visto dal basso  e 0 visto difronte\n",
        "  #azimnut da 0 a 360         Fa ruotare  0 visto difronte  360 visto da dietro      -120 visto di lato\n",
        "  #angoli da 0 a 360\n",
        "    images = [ resize(render_point_cloud(point_cloud,3,90,-120,0,0,0),res)\n",
        "\n",
        "\n",
        "    ]\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    return images\n",
        "\n",
        "\n",
        "def generate_random_view(point_cloud, res, n_views, std=1):\n",
        "  elev = random.sample(range(-90, 91), n_views)  # Genera 4 elevazioni casuali tra -90 e 90\n",
        "  azimut = random.sample(range(0, 361), n_views)\n",
        "  images = []\n",
        "  for i in range(n_views):\n",
        "      images.append(resize(render_point_cloud(point_cloud,3,elev[i],azimut[i],0,0,0),res))\n",
        "\n",
        "  images = torch.stack(images)\n",
        "  return images\n",
        "\n"
      ],
      "metadata": {
        "id": "6bOG9VHnFk4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "\n",
        "import open3d as o3d\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def segment2rgb(pred_class, colors):\n",
        "    pred_rgb = torch.zeros(pred_class.shape[0], 3).to(device)\n",
        "    for class_idx, color in enumerate(colors):\n",
        "        pred_rgb += torch.matmul(pred_class[:,class_idx].unsqueeze(1), color.unsqueeze(0))\n",
        "\n",
        "    return pred_rgb\n",
        "\n",
        "def color_point_cloud(pred_class, verts, colors):\n",
        "    pred_rgb = segment2rgb(pred_class, colors)\n",
        "    #print(pred_rgb.shape)\n",
        "    #print(verts.shape[0])\n",
        "\n",
        "    return Pointclouds(points=[verts], features=[pred_rgb])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5YtYt4U6NNmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade pip\n",
        "!git clone https://github.com/rasenqt/AML_Project_2024.git\n",
        "#!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n"
      ],
      "metadata": {
        "id": "xZ8AyvM6_AmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZE POINT CLOUD\n"
      ],
      "metadata": {
        "id": "XATE3-AzioFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "sys.path.append('/content/AML_Project_2024/3DHighlighter-main')\n",
        "from itertools import permutations, product\n",
        "from neural_highlighter import NeuralHighlighter\n",
        "from Normalization import MeshNormalizer,PointCloudNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "import open3d as o3d\n",
        "\n",
        "\n",
        "\n",
        "def optimize_point_cloud(args):\n",
        "    # Constrain most sources of randomness\n",
        "    # (some torch backwards functions within CLIP are non-determinstic)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Load CLIP model\n",
        "    clip_model, preprocess = clip.load(args.clipmodel, device, jit=args.jit)\n",
        "\n",
        "    # Adjust output resolution depending on model type\n",
        "    res = 224\n",
        "    if args.clipmodel == \"ViT-L/14@336px\":\n",
        "        res = 336\n",
        "    if args.clipmodel == \"RN50x4\":\n",
        "        res = 288\n",
        "    if args.clipmodel == \"RN50x16\":\n",
        "        res = 384\n",
        "    if args.clipmodel == \"RN50x64\":\n",
        "        res = 448\n",
        "\n",
        "    Path(os.path.join(args.output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "    pointcloud = o3d.io.read_point_cloud(args.obj_path)\n",
        "    verts = torch.Tensor(np.asarray(pointcloud.points)).to(device)\n",
        "\n",
        "    print(\"Verts shape\")\n",
        "    print(verts.shape)\n",
        "\n",
        "\n",
        "    # Manually set RGB color (assuming you want to set all points to red)\n",
        "    manual_color = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32, device=device)\n",
        "\n",
        "    # Repeat the manual color for each vertex\n",
        "    rgb_colors = manual_color.repeat(verts.shape[0], 1)\n",
        "\n",
        "    shift = torch.mean(verts, dim=0)\n",
        "    scale = torch.max(torch.norm(verts-shift, p=2, dim=1))\n",
        "    verts=(verts-shift)/scale\n",
        "\n",
        "    point_cloud = Pointclouds(points=[verts], features=[rgb_colors])\n",
        "    #PointCloudNormalizer(point_cloud)()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize variables\n",
        "    background = None\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    n_augs = args.n_augs\n",
        "    dir = args.output_dir\n",
        "\n",
        "    # Record command line arguments\n",
        "    with open(os.path.join(dir, 'commandline_args.txt'), 'w') as f:\n",
        "        json.dump(args.__dict__, f, indent=2)\n",
        "\n",
        "    # CLIP and Augmentation Transforms\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # MLP Settings\n",
        "    mlp = NeuralHighlighter(args.depth, args.width, out_dim=args.n_classes, positional_encoding=args.positional_encoding,\n",
        "                            sigma=args.sigma).to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), args.learning_rate)\n",
        "\n",
        "    # list of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "    # --- Prompt ---\n",
        "    # pre-process multi_word_inputs\n",
        "    args.object[0] = ' '.join(args.object[0].split('_'))\n",
        "    for i in range(len(args.classes)):\n",
        "        args.classes[i] = ' '.join(args.classes[i].split('_'))\n",
        "    # encode prompt with CLIP\n",
        "    prompt = \"Point cloud depth map of gray {} with highlighted {}\".format(args.object[0], args.classes[0])\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(verts)\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(args.n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        verts_curr = vertices\n",
        "        colored_point_cloud = color_point_cloud(pred_class, verts_curr, colors)\n",
        "        rendered_images = generate_static_views(colored_point_cloud, res)\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(dir, i, rendered_images)\n",
        "            with open(os.path.join(dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "    # re-initialize background color\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    # save results\n",
        "    #save_final_results(args, dir, vertices, mlp, vertices, colors, render, background)\n",
        "\n",
        "    # Save prompts\n",
        "    with open(os.path.join(dir, prompt), \"w\") as f:\n",
        "        f.write('')\n",
        "\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(args, dir, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=args.n_views,\n",
        "                                                                        show=args.show,\n",
        "                                                                        center_azim=args.frontview_center[0],\n",
        "                                                                        center_elev=args.frontview_center[1],\n",
        "                                                                        std=args.frontview_std,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "        mesh.export(os.path.join(dir, f\"{objbase}_{args.classes[0]}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if args.n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if args.clipavg == \"view\":\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "        else:\n",
        "            loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    elif args.n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(args.n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if args.clipavg == \"view\":\n",
        "                if encoded_text.shape[0] > 1:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "                else:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "            else:\n",
        "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "y59DBoOSteAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZE MESH"
      ],
      "metadata": {
        "id": "p1efWNPoiuNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from itertools import permutations, product\n",
        "from neural_highlighter import NeuralHighlighter\n",
        "from Normalization import MeshNormalizer,PointCloudNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "\n",
        "\n",
        "def optimize(args):\n",
        "    # Constrain most sources of randomness\n",
        "    # (some torch backwards functions within CLIP are non-determinstic)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Load CLIP model\n",
        "    clip_model, preprocess = clip.load(args.clipmodel, device, jit=args.jit)\n",
        "\n",
        "    # Adjust output resolution depending on model type\n",
        "    res = 224\n",
        "    if args.clipmodel == \"ViT-L/14@336px\":\n",
        "        res = 336\n",
        "    if args.clipmodel == \"RN50x4\":\n",
        "        res = 288\n",
        "    if args.clipmodel == \"RN50x16\":\n",
        "        res = 384\n",
        "    if args.clipmodel == \"RN50x64\":\n",
        "        res = 448\n",
        "\n",
        "    Path(os.path.join(args.output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "\n",
        "    render = Renderer(dim=(args.render_res, args.render_res))\n",
        "    mesh = Mesh(args.obj_path)\n",
        "    MeshNormalizer(mesh)()\n",
        "\n",
        "    # Initialize variables\n",
        "    background = None\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    n_augs = args.n_augs\n",
        "    dir = args.output_dir\n",
        "\n",
        "    # Record command line arguments\n",
        "    with open(os.path.join(dir, 'commandline_args.txt'), 'w') as f:\n",
        "        json.dump(args.__dict__, f, indent=2)\n",
        "\n",
        "    # CLIP and Augmentation Transforms\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # MLP Settings\n",
        "    mlp = NeuralHighlighter(args.depth, args.width, out_dim=args.n_classes, positional_encoding=args.positional_encoding,\n",
        "                            sigma=args.sigma).to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), args.learning_rate)\n",
        "\n",
        "    # list of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "    # --- Prompt ---\n",
        "    # pre-process multi_word_inputs\n",
        "    args.object[0] = ' '.join(args.object[0].split('_'))\n",
        "    for i in range(len(args.classes)):\n",
        "        args.classes[i] = ' '.join(args.classes[i].split('_'))\n",
        "    # encode prompt with CLIP\n",
        "    prompt = \"A 3D render of a gray {} with highlighted {}\".format(args.object[0], args.classes[0])\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(mesh.vertices)\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(args.n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        sampled_mesh = mesh\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "        rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=args.n_views,\n",
        "                                                                show=args.show,\n",
        "                                                                center_azim=args.frontview_center[0],\n",
        "                                                                center_elev=args.frontview_center[1],\n",
        "                                                                std=args.frontview_std,\n",
        "                                                                return_views=True,\n",
        "                                                                lighting=True,\n",
        "                                                                background=background)\n",
        "\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(dir, i, rendered_images)\n",
        "            with open(os.path.join(dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "    # re-initialize background color\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    # save results\n",
        "    save_final_results(args, dir, mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "    # Save prompts\n",
        "    with open(os.path.join(dir, prompt), \"w\") as f:\n",
        "        f.write('')\n",
        "\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(args, dir, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=args.n_views,\n",
        "                                                                        show=args.show,\n",
        "                                                                        center_azim=args.frontview_center[0],\n",
        "                                                                        center_elev=args.frontview_center[1],\n",
        "                                                                        std=args.frontview_std,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "        mesh.export(os.path.join(dir, f\"{objbase}_{args.classes[0]}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if args.n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if args.clipavg == \"view\":\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "        else:\n",
        "            loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    elif args.n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(args.n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if args.clipavg == \"view\":\n",
        "                if encoded_text.shape[0] > 1:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "                else:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "            else:\n",
        "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
      ],
      "metadata": {
        "id": "Ya33v0xMA5l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZE AFFORDANCE"
      ],
      "metadata": {
        "id": "ROFtCSSXjDZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "sys.path.append('/content/AML_Project_2024/3DHighlighter-main')\n",
        "from itertools import permutations, product\n",
        "from neural_highlighter import NeuralHighlighter\n",
        "from Normalization import MeshNormalizer,PointCloudNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "import open3d as o3d\n",
        "\n",
        "\n",
        "\n",
        "def optimize_affordance(args, verts):\n",
        "    # Constrain most sources of randomness\n",
        "    # (some torch backwards functions within CLIP are non-determinstic)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Load CLIP model\n",
        "    clip_model, preprocess = clip.load(args.clipmodel, device, jit=args.jit)\n",
        "\n",
        "    # Adjust output resolution depending on model type\n",
        "    res = 224\n",
        "    if args.clipmodel == \"ViT-L/14@336px\":\n",
        "        res = 336\n",
        "    if args.clipmodel == \"RN50x4\":\n",
        "        res = 288\n",
        "    if args.clipmodel == \"RN50x16\":\n",
        "        res = 384\n",
        "    if args.clipmodel == \"RN50x64\":\n",
        "        res = 448\n",
        "\n",
        "    Path(os.path.join(args.output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "    verts = verts.to(device)\n",
        "\n",
        "    print(\"Verts shape\")\n",
        "    print(verts.shape)\n",
        "\n",
        "\n",
        "    # Manually set RGB color (assuming you want to set all points to red)\n",
        "    manual_color = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float32, device=device)\n",
        "\n",
        "    # Repeat the manual color for each vertex\n",
        "    rgb_colors = manual_color.repeat(verts.shape[0], 1)\n",
        "\n",
        "    shift = torch.mean(verts, dim=0)\n",
        "    scale = torch.max(torch.norm(verts-shift, p=2, dim=1))\n",
        "    verts=(verts-shift)/scale\n",
        "\n",
        "    point_cloud = Pointclouds(points=[verts], features=[rgb_colors])\n",
        "    #PointCloudNormalizer(point_cloud)()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize variables\n",
        "    background = None\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    n_augs = args.n_augs\n",
        "    dir = args.output_dir\n",
        "\n",
        "    # Record command line arguments\n",
        "    with open(os.path.join(dir, 'commandline_args.txt'), 'w') as f:\n",
        "        json.dump(args.__dict__, f, indent=2)\n",
        "\n",
        "    # CLIP and Augmentation Transforms\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # MLP Settings\n",
        "    mlp = NeuralHighlighter(args.depth, args.width, out_dim=args.n_classes, positional_encoding=args.positional_encoding,\n",
        "                            sigma=args.sigma).to(device)\n",
        "    momentum_value = 0.9  # Puoi regolare il valore del momentum\n",
        "    #optim = torch.optim.Adam(mlp.parameters(), lr=args.learning_rate, betas=(momentum_value, 0.999))\n",
        "    optim = optim.SGD(mlp.parameters(), lr=args.learning_rate, momentum=0.9)\n",
        "\n",
        "    #optim = torch.optim.Adam(mlp.parameters(), args.learning_rate)\n",
        "\n",
        "    # list of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    #full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    full_colors = [[204/255, 1., 0.], [0., 0.,0.]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    #rgb_to_color = {(0, 0., 255): \"blue\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    #color_to_rgb = {\"blue\": [0, 0., 255.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    #full_colors = [[0, 0., 255.], [180/255, 180/255, 180/255]]\n",
        "    #colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    #rgb_to_color = {(0, 0., 1.): \"blue\", (204/255, 1., 0.): \"highlighter\"}\n",
        "    #color_to_rgb = {\"blue\": [0, 0., 1.], \"highlighter\": [204/255, 1., 0.]}\n",
        "    #full_colors = [[1., 0., 1.],[204/255, 1., 0.]]\n",
        "    #colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # --- Prompt ---\n",
        "    # pre-process multi_word_inputs\n",
        "    args.object[0] = ' '.join(args.object[0].split('_'))\n",
        "    for i in range(len(args.classes)):\n",
        "        args.classes[i] = ' '.join(args.classes[i].split('_'))\n",
        "    # encode prompt with CLIP\n",
        "    #prompt = \"If you were to open the Door, from which point on the door would you open it?\"\n",
        "    #prompt = \"A point cloud representing a gray {} with highlighted  handle\".format(args.object[0], args.classes[0])\n",
        "    #prompt = \"A point cloud representing a gray {} with highlighted  legs\".format(args.object[0])\n",
        "    #prompt =\"point cloud depth map of a gray {} with hightlighted legs\".format(args.object[0])\n",
        "    prompt =\"point cloud depth map of a {} with hightlighted legs\".format(args.object[0])\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = clip_model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(verts)\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(args.n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        verts_curr = vertices\n",
        "        colored_point_cloud = color_point_cloud(pred_class, verts_curr, colors)\n",
        "        rendered_images =  generate_random_view(colored_point_cloud, res, args.n_views)\n",
        "        #rendered_images =  generate_static_views(colored_point_cloud, res)\n",
        "\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(dir, i, rendered_images)\n",
        "            with open(os.path.join(dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "    # re-initialize background color\n",
        "    if args.background is not None:\n",
        "        assert len(args.background) == 3\n",
        "        background = torch.tensor(args.background).to(device)\n",
        "    # save results\n",
        "    #save_final_results(args, dir, vertices, mlp, vertices, colors, render, background)\n",
        "\n",
        "    # Save prompts\n",
        "    with open(os.path.join(dir, prompt), \"w\") as f:\n",
        "        f.write('')\n",
        "\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(args, dir, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=args.n_views,\n",
        "                                                                        show=args.show,\n",
        "                                                                        center_azim=args.frontview_center[0],\n",
        "                                                                        center_elev=args.frontview_center[1],\n",
        "                                                                        std=args.frontview_std,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        objbase, extension = os.path.splitext(os.path.basename(args.obj_path))\n",
        "        mesh.export(os.path.join(dir, f\"{objbase}_{args.classes[0]}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(args, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if args.n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if args.clipavg == \"view\":\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "        else:\n",
        "            loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    elif args.n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(args.n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if args.clipavg == \"view\":\n",
        "                if encoded_text.shape[0] > 1:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "                else:\n",
        "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "            else:\n",
        "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n",
        "\n"
      ],
      "metadata": {
        "id": "7yawqTO-jNpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN AFFORDANCE"
      ],
      "metadata": {
        "id": "jUDtbwWpkFmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "#sys.path.append('/content/AML_Project_2024/3DHighlighter-main/')\n",
        "#from main import optimize # Import the function from the module\n",
        "\n",
        "class ArgsNamespace:\n",
        "    def __init__(self, args_dict):\n",
        "        self.__dict__.update(args_dict)\n",
        "\n",
        "# Create an instance of ArgsNamespace\n",
        "\n",
        "\n",
        "\n",
        "# specify the directory you want to iterate over\n",
        "#directory = './AML_Project_2024/point_cloud_dataset/'  # current directory\n",
        "#directory= './AML_Project_2024/dataset/'\n",
        "directory = './AML_Project_2024/full_shape_val_data/Chair/'\n",
        "# iterate over files in the directory\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "    with open(directory + filename, 'rb') as f: ## da definire...\n",
        "      # Load the object from the file\n",
        "      temp_data = pkl.load(f)\n",
        "      # print((temp_data[0]))\n",
        "      verts = temp_data[10][\"full_shape\"][\"coordinate\"]\n",
        "      verts = torch.Tensor(np.asarray(verts))\n",
        "\n",
        "\n",
        "    object_name = temp_data[10][\"semantic class\"]\n",
        "    print(object_name)\n",
        "\n",
        "\n",
        "    args = ArgsNamespace({\n",
        "        'seed': 0,\n",
        "        'obj_path': directory + filename,\n",
        "        'output_dir': '/content/ooo' + object_name,\n",
        "        'object': [object_name],\n",
        "        'classes': [\"knife\"],\n",
        "        'background': [1., 1., 1.],\n",
        "        'n_views': 5,\n",
        "        'frontview_std': 4,\n",
        "        'frontview_center': [0., 0.],\n",
        "        'show': False,\n",
        "        'n_augs': 3,\n",
        "        'clipavg': 'view',\n",
        "        'render_res': 224,\n",
        "        'clipmodel': 'ViT-L/14',\n",
        "        'jit': False,\n",
        "        'depth': 4,\n",
        "        'width': 256,\n",
        "        'n_classes': 2,\n",
        "        'positional_encoding': False,\n",
        "        'sigma': 3.0,\n",
        "        'learning_rate': 0.0001,\n",
        "        'n_iter': 2500\n",
        "    })\n",
        "\n",
        "\n",
        "    optimize_affordance(args, verts)\n",
        "    break\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/results_television.zip /content/results_television\n",
        "files.download('/content/results_television.zip')\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m7fj4_qkf_I8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}